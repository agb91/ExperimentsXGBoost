from  __future__ import division
from gene import Gene
from gene_creator import GeneCreator
from titanic_boost_regressor import TitanicBoostRegressor
from titanic_boost_classifier import TitanicBoostClassifier
import numpy as np
import random
import math
from data_reader import DataReader
from various_forests import VariousForests


'''
how does a genetic algorithm work? 
https://en.wikipedia.org/wiki/Genetic_algorithm
'''
class Breeder:

	# create a new generation made of:
	#	1) best elements of the old generation
	#	2) new elements generated from the best old ones (sons)
	#	3) some randome new elements, in order to add some variability
	def get_new_generation( self, old, n):
		gene_creator = GeneCreator()
		new_generation = list()
		strongest_n = 3
		if(n<3):
			strongest_n = n
		reprods = math.ceil(n/2.5)
		random_adds = 2 
		goods = self.take_goods( old , strongest_n )
		reproducible = self.take_goods(old, reprods )
		#I want to maintain old goods in my genetic pools
		for i in range( 0, len(goods) ):
			new_generation.append(goods[i])
		#I want some sons generated by goods
		for i in range( 0 , (n - strongest_n - random_adds ) ):
			son = self.get_son( reproducible )
			new_generation.append(son)
		#I want also some randoms new borns
		for i in range( 0, random_adds ):
			new_generation.append( gene_creator.random_create() )
		
		return new_generation

	# from parent genes (not necessarily 2 parents.. maybe more but at least 1) generate
	# a new gene with attributes takan randomly from parents
	def get_son( self, parents ):

		cbti = random.randint(0, (len(parents) - 1 ) )
		cbt = parents[cbti].col_by_tree 
		
		ssi = random.randint(0, (len(parents) - 1 ))
		ss = parents[ssi].subsample 

		mcwi = random.randint(0, (len(parents) - 1 ))
		mcw = parents[mcwi].min_child_weight 

		mdi = random.randint(0, (len(parents) - 1 ))
		md = parents[mdi].max_depth 

		nei  = random.randint(0, (len(parents) - 1 ))
		ne = parents[nei].n_estimators 

		lri = random.randint(0, (len(parents) - 1 ))
		lr = parents[lri].learning_rate 

		wayi = random.randint(0, (len(parents) - 1 ))
		way = parents[wayi].way

		nnei = random.randint( 0, (len(parents) - 1 ))
		n_neighbors = parents[nnei].way

		son = Gene( cbt, ss, mcw, md, ne, lr, way , n_neighbors)
		
		return son	

		
	#the first is completely random..
	def get_first_generation( self, n ):
		genes = list()
		creator = GeneCreator()
		for i in range( 0 , n):
			g = creator.random_create()
			genes.append(g)
		return genes

	#from the best to the worst, according to a fitness function able to evaluate the "level" of the gene
	def order_genes( self , genes ):
		result = []
		genes_set = set(genes)
		genes = list( genes_set ) # no doubles!
		#for i in range( 0, len(genes) ):
		#	print( "before: " + str(genes[i].level) )		
		
		result = sorted(genes, key=lambda x: x.level, reverse=True)
		#for i in range( 0, len(result) ):
		#	print( result[i].level )		
		
		return result

	# take the best N elements
	def take_goods( self, genes, n ):
		goods = []

		for i in range(0, len(genes) ):
			g = genes[i]
			goods.append(g)
			goods = self.order_genes( goods )
			if( len( goods ) > n):
				goods = goods[ 0 : n ]

		#for i in range( 0, len(goods) ):
		#	print( goods[i].level )		
		return goods		    

	#  take the best 1 element
	def take_best( self, genes ):
		return self.take_goods( genes, 1 )[0]

	# run the algorithm itself
	# generation is the population of genes in this iteration
	def run(self, generation):
		runned_generation = list()
		data_reader = DataReader.getInstance()
		X,Y,X_test,X_output = data_reader.read_data()

		#for each gene of this generation
		for i in range( 0 , len(generation)):
			
			this_gene = generation[i]
			# runner is which algorithm will I use:
			# 0 is XGBoost Classifier
			# 1 is XGBoost regressor
			# 2 is SVC
			# 3 is DecisionTreeClassifier
			# 4 is AdaBoost applied to DecisionTreeClassifier
			# 5 is GradientBoosting
			# 6 is KNeighbors
			# 7 is RandomForest
			# 8 is RandomForest but simplified (more defaults and less configuration)
			runner = None
			if( this_gene.way == 0 ):
				runner = TitanicBoostClassifier()
			else:
				if( this_gene.way == 1 ):
					runner = TitanicBoostRegressor()
				else:
					runner = VariousForests()	

			runner.set_datasets( X , Y , X_test , X_output )
			runner.set_gene_to_model( this_gene ) #here we configure the model
			this_gene.set_fitness_level( runner.run() ) 
			runned_generation.append(this_gene)

		return runned_generation